from google import genai
import edge_tts
from tqdm import tqdm
import time
import itertools
import os
import torch
import numpy as np
from utility.text import *
#from kokoro import KPipeline
import soundfile as sf
import textwrap


'''
async def kokoro_tts_example(text, output_dir, filename, voice="zm_yunyang"):
    """
    Generates speech from text using Kokoro's KPipeline and saves it to a specific directory.
    
    :param text: Text to be converted to speech.
    :param output_dir: Directory where the audio file will be saved.
    :param filename: Name of the audio file.
    :param voice: The voice model to use.
    :return: Full path of the saved audio file.
    """
    if not text or text.strip() == "":
        print("‚ö†Ô∏è Warning: Received empty text for speech synthesis.")
        return None  # Skip empty text

    # Ensure the output directory exists
    os.makedirs(output_dir, exist_ok=True)
    
    # Define the full file path (using .wav for this example)
    output_file_path = os.path.join(output_dir, filename)
    
    # Initialize the KPipeline with an explicit repo_id to suppress the warning
    pipeline_instance = KPipeline(lang_code='z', repo_id="hexgrad/Kokoro-82M")
    
    try:
        print(f"üîä Generating speech using voice: {voice}")
        # Create a generator that yields (graphemes, phonemes, audio)
        generator = pipeline_instance(
            text, voice=voice, speed=1, split_pattern=r'\n+'
        )
        
        # Merge audio segments (convert Tensors to NumPy arrays if needed)
        audio_segments = []
        for i, (_, _, audio) in enumerate(generator):
            # Check if the audio segment is a Tensor and convert it to a NumPy array
            if isinstance(audio, torch.Tensor):
                audio_np = audio.detach().cpu().numpy()
            else:
                audio_np = audio
            audio_segments.append(audio_np)
        
        if audio_segments:
            # Concatenate the segments along the first dimension
            final_audio = np.concatenate(audio_segments, axis=0)
            sf.write(output_file_path, final_audio, 24000)
            print(f"üíæ Saved TTS audio to: {output_file_path}")
            return output_file_path
        else:
            print("‚ö†Ô∏è No audio was generated.")
            return None

    except Exception as e:
        print(f"‚ùå Error generating speech: {e}")
        return None
'''

async def edge_tts_example(text, output_dir, filename, voice="en-US-KaiNeural"):
    """
    Generates speech from text and saves it to a specific directory.
    
    :param text: Text to be converted to speech.
    :param output_dir: Directory where the audio file will be saved.
    :param filename: Name of the audio file.
    :param voice: The voice model to use.
    :return: Full path of the saved audio file.
    """
    if not text or text.strip() == "":
        print("‚ö†Ô∏è Warning: Received empty text for speech synthesis.")
        return None  # Skip empty text

    # ‚úÖ Ensure the output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # ‚úÖ Define the full file path
    output_file_path = os.path.join(output_dir, filename)

    # ‚úÖ Generate speech and save it to the file
    communicate = edge_tts.Communicate(text, voice, rate="+20%")
    
    try:
        print(f"üíæ Saving TTS audio to: {output_file_path}")
        await communicate.save(output_file_path)
    except Exception as e:
        print(f"‚ùå Error saving audio: {e}")
        return None

    return output_file_path  # Return the saved file path



def gemini_chat(text_array=None, script=None, clients=None, keys=None, max_retries=100):
    if text_array is None or script is None:
        raise ValueError("script or text_array can't be None")
    
    if (clients is None or len(clients) == 0) and (keys is None or len(keys) == 0):
        raise ValueError("Either clients or keys must be provided")

    # ‚úÖ If only keys are provided, create clients
    if clients is None or len(clients) == 0:
        clients = [genai.Client(api_key=key) for key in keys]

    # ‚úÖ Ensure we have multiple clients to rotate
    client_cycle = itertools.cycle(clients)

    response_array_of_text = []
    count = 0
    for idx, text in enumerate(tqdm(text_array)):
        retries = 0
        client = next(client_cycle)  # Get the first client

        while retries < max_retries:
            try:
                response = client.models.generate_content(
                    model="gemini-2.0-flash",
                    contents=textwrap.dedent(f'''\
                    ‰ª•‰∏ãÊòØÊàëÂÄëÁöÑÂÆåÊï¥Ë¨õÁ®øÔºö{script}  
                    ‰ª•‰∏ãÊòØÁ¨¨ {count} Âºµ Ptt ÂÖßÂÆπÔºåÂâçÈù¢ÂπæÂºµÂ∑≤Á∂ìËôïÁêÜÂÆåÁï¢Ôºö{text}  
                    Ê†πÊìö‰∏äËø∞Ë≥áÊñôÔºå‰∏¶Âæû‰∏≠ËêÉÂèñËàáÊ≠§ÂºµÊäïÂΩ±ÁâáÁõ¥Êé•Áõ∏ÈóúÁöÑÈáçÈªûÔºåÁîüÊàê‰∏ÄÊÆµÈáùÂ∞çË©≤ÊäïÂΩ±ÁâáÁöÑË¨õÁ®øÔºåÊØèÊÆµË¨õÁ®øÂÑòÈáèÂú® 15 ÁßíÂÖßË¨õÂÆå„ÄÇ  
                    Ë¶ÅÊ±ÇÂ¶Ç‰∏ãÔºö  
                    1. ‰Ω†ÊòØ‰∏Ä‰ΩçË¨õËÄÖÔºåÁî®ÂÉè‰∫∫Ë¨õË©±ÁöÑÊñπÂºèÊñπÂºè„ÄÇ   
                    2. Áõ¥Êé•ÈñãÂßãÁîüÊàêÂÖßÂÆπÔºå‰∏çË¶ÅÈñãÈ†≠Ëàá„ÄÅÈñãÂ†¥ÁôΩÔºå‰∏çË¶ÅÂá∫Áèæ„ÄåÂ•ΩÁöÑ„Äç„ÄÅ„ÄåÊàëÂÄë‰æÜÁúãÁ¨¨ÂπæÂºµÊäïÂΩ±Áâá„Äç„ÄÇ
                    '''),

                )
                response_array_of_text.append(remove_markdown(response.text))
                count += 1
                break  # ‚úÖ Successful request; exit retry loop
            except Exception as e:
                error_message = str(e)
                if "RESOURCE_EXHAUSTED" in error_message:
                    wait_time = 2 ** retries  # Exponential backoff
                    print(f"Rate limit reached for current client. Switching client and retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
                    retries += 1
                    client = next(client_cycle)  # üîÑ Rotate to the next client
                else:
                    raise e  # ‚ö†Ô∏è Other errors should not be retried (e.g., invalid request)
        else:
            raise Exception("Max retries reached. Aborting.")
    
    return response_array_of_text



'''\
‰ª•‰∏ãÊòØÊàëÂÄëÁöÑÂÆåÊï¥Ë¨õÁ®øÔºö{script}  
‰ª•‰∏ãÊòØÁ¨¨ {count} Âºµ Ptt ÂÖßÂÆπÔºåÂâçÈù¢ÂπæÂºµÂ∑≤Á∂ìËôïÁêÜÂÆåÁï¢Ôºö{text}  
Ê†πÊìö‰∏äËø∞Ë≥áÊñôÔºå‰∏¶Âæû‰∏≠ËêÉÂèñËàáÊ≠§ÂºµÊäïÂΩ±ÁâáÁõ¥Êé•Áõ∏ÈóúÁöÑÈáçÈªûÔºåÁîüÊàê‰∏ÄÊÆµÈáùÂ∞çË©≤ÊäïÂΩ±ÁâáÁöÑË¨õÁ®øÔºåÊØèÊÆµË¨õÁ®øÂÑòÈáèÂú® 15 ÁßíÂÖßË¨õÂÆå„ÄÇ  
Ë¶ÅÊ±ÇÂ¶Ç‰∏ãÔºö  
1. ‰Ω†ÊòØ‰∏Ä‰ΩçË¨õËÄÖÔºåÁî®ÂÉè‰∫∫Ë¨õË©±ÁöÑÊñπÂºèÊñπÂºè„ÄÇ   
2. Áõ¥Êé•ÈñãÂßãÁîüÊàêÂÖßÂÆπÔºå‰∏çË¶ÅÈñãÈ†≠Ëàá„ÄÅÈñãÂ†¥ÁôΩÔºå‰∏çË¶ÅÂá∫Áèæ„ÄåÂ•ΩÁöÑ„Äç„ÄÅ„ÄåÊàëÂÄë‰æÜÁúãÁ¨¨ÂπæÂºµÊäïÂΩ±Áâá„Äç„ÄÇ
'''

"""\
                        Here is our full script:
                        {script}

                        Here is the content from PTT for slide {idx+1}; previous slides have been processed:
                        {text}

                        Based on above, extract the key points directly related to this slide and generate
                        a ~15-second speaker script segment.

                        Requirements:
                        1. You are a presenter, speaking in a natural, conversational manner.
                        2. Keep segments short (‚âà15 seconds).
                        3. Do not include opening remarks or ‚ÄúOkay,‚Äù ‚ÄúLet‚Äôs look at slide X,‚Äù etc.
                        4. Begin generating immediately.
                        """
